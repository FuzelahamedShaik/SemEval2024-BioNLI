{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6a4dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import AutoTokenizer, GPT2Tokenizer\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c4ae7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SepToken = {'bert': \"[SEP]\", 'roberta': \"</s>\", 'electra': \"[SEP]\", 'deberta': \"[SEP]\", 'bart': \"<s>\",\n",
    "            'gpt2': ''} \n",
    "Label2num = {'Entailment': 1, \"Contradiction\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe257713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recorddataset(Dataset):\n",
    "    def __init__(self, args, path, split='train'):\n",
    "        super(Recorddataset, self).__init__()\n",
    "        self.args = args\n",
    "        self.split = split\n",
    "        self.statement = []\n",
    "        self.trail1 = []\n",
    "        self.trail2 = []\n",
    "        self.label = []\n",
    "        self.section = []\n",
    "        ctpath = path + \"/CT json/\"\n",
    "        jspath = path + f\"/{split}\" + \".json\" if split != 'trn&dev' else path + '/train.json'\n",
    "        with open(jspath) as file:\n",
    "            self.data = json.load(file)\n",
    "            self.uuid_list = list(self.data.keys())\n",
    "        if split == 'trn&dev':  \n",
    "            with open(path + '/dev.json') as file:\n",
    "                self.data = {**self.data, **json.load(file)}  \n",
    "                self.uuid_list = list(self.data.keys())\n",
    "        for id in self.uuid_list:\n",
    "            self.statement.append(self.data[id]['Statement'])\n",
    "            if split != 'test':\n",
    "                self.label.append(Label2num[self.data[id]['Label']])\n",
    "            section = self.data[id]['Section_id']\n",
    "            self.section.append(section)\n",
    "            with open(\n",
    "                    ctpath + f\"{self.data[id]['Primary_id']}\" + \".json\") as file: \n",
    "                ct = json.load(file)\n",
    "                trail1 = ct[section]\n",
    "                self.trail1.append(self.format_change(trail1))\n",
    "            if self.data[id]['Type'] == \"Comparison\":  \n",
    "                with open(ctpath + f\"{self.data[id]['Secondary_id']}\" + \".json\") as file:\n",
    "                    ct = json.load(file)\n",
    "                    trail2 = ct[section]\n",
    "                    self.trail2.append(self.format_change(trail2))\n",
    "            else:\n",
    "                self.trail2.append(\"_\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.args['prompt'] == 2:\n",
    "            if self.trail2[index] == '_':\n",
    "                sent = \"{} [SEP] {} [SEP] {}\".format(self.statement[index], self.section[index],\n",
    "                                                     self.trail1[index])\n",
    "            else:\n",
    "                sent = \"{} [SEP] {} [SEP] {} [SEP] {}\".format(self.statement[index], self.section[index],\n",
    "                                                              self.trail1[index], self.trail2[index])\n",
    "        elif self.args['prompt'] == 1:\n",
    "            if self.trail2[index] == '_':\n",
    "                if 'gpt' in self.args['lmn']:\n",
    "                    sent = \"{}, {}, {} <|endoftext|>\".format(self.statement[index],\n",
    "                                                             f\"the {self.section[index]} clue of first trail is: \",\n",
    "                                                             self.trail1[index])\n",
    "\n",
    "                elif 'bart' in self.args['lmn']:\n",
    "                    sent = \"{} {} {} {} {}\".format(self.statement[index], SepToken[self.args['lmn']],\n",
    "                                                   f\"the {self.section[index]} clue of first trail is: \",\n",
    "                                                   SepToken[self.args['lmn']],\n",
    "                                                   self.trail1[index],\n",
    "                                                   '')\n",
    "                else:\n",
    "                    sent = \"{} {} {} {} {} {}\".format(self.statement[index],\n",
    "                                                      SepToken[self.args['lmn']],\n",
    "                                                      f\"the {self.section[index]} clue of first trail is: \",\n",
    "                                                      SepToken[self.args['lmn']],\n",
    "                                                      self.trail1[index],\n",
    "                                                      SepToken[self.args['lmn']])\n",
    "            else:\n",
    "                if 'gpt' in self.args['lmn']:\n",
    "                    sent = \"{}, {}, {}, {}, {} <|endoftext|>\".format(self.statement[index],\n",
    "                                                                     f\"the {self.section[index]} clue of first trail is: \",\n",
    "                                                                     self.trail1[index],\n",
    "                                                                     f\"the {self.section[index]} clue of second trail is: \",\n",
    "                                                                     self.trail2[index], )\n",
    "                elif 'bart' in self.args['lmn']:\n",
    "                    sent = \"{} {} {} {} {} {} {} {} {} {}\".format(self.statement[index], SepToken[self.args['lmn']],\n",
    "                                                                  f\"the {self.section[index]} clue of first trail is: \",\n",
    "                                                                  SepToken[self.args['lmn']],\n",
    "                                                                  self.trail1[index],\n",
    "                                                                  SepToken[self.args['lmn']],\n",
    "                                                                  f\"the {self.section[index]} clue of second trail is: \",\n",
    "                                                                  SepToken[self.args['lmn']],\n",
    "                                                                  self.trail2[index],\n",
    "                                                                  # </s>\n",
    "                                                                  '')\n",
    "                else:\n",
    "                    sent = \"{} {} {} {} {} {} {} {} {} {}\".format(self.statement[index], SepToken[self.args['lmn']],\n",
    "                                                                  f\"the {self.section[index]} clue of first trail is: \",\n",
    "                                                                  SepToken[self.args['lmn']],\n",
    "                                                                  self.trail1[index],\n",
    "                                                                  SepToken[self.args['lmn']],\n",
    "                                                                  f\"the {self.section[index]} clue of second trail is: \",\n",
    "                                                                  SepToken[self.args['lmn']],\n",
    "                                                                  self.trail2[index],\n",
    "                                                                  SepToken[self.args['lmn']])\n",
    "\n",
    "        elif self.args['prompt'] == 0:  \n",
    "            sent = \"{}, {}, {}, {}, {}\".format(self.statement[index],\n",
    "                                               f\"the {self.section[index]} clue of first trail is: \",\n",
    "                                               self.trail1[index],\n",
    "                                               f\"the {self.section[index]} clue of second trail is: \",\n",
    "                                               self.trail2[index], )\n",
    "\n",
    "        elif self.args['prompt'] == 3:\n",
    "            sent = \"{}\".format(self.statement[index])\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(\"Prompt not implemented\")\n",
    "        if self.split != 'test':\n",
    "            return sent, torch.tensor(self.label[index])\n",
    "        else:\n",
    "            return sent, self.uuid_list[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.uuid_list)\n",
    "\n",
    "    def format_change(self, sentence):\n",
    "        s = \"\"\n",
    "        for sent in sentence:\n",
    "            s += sent.strip() + \",\"\n",
    "        return s\n",
    "\n",
    "    def get_max_length(self):\n",
    "        print([len(self.__getitem__(i)[0].split(' ')) for i in range(self.__len__())])\n",
    "        return max([len(self.__getitem__(i)[0].split(' ')) for i in range(self.__len__())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fae10bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self,args,model_name,from_check_point = False,tokenizer_dir = None, model_dir = None):\n",
    "        super(Model,self).__init__()\n",
    "        assert(type(from_check_point) == bool)   \n",
    "\n",
    "        self.args = args\n",
    "        if 'gpt' in model_name:\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(model_name,do_lower_case = True) if from_check_point == False else GPT2Tokenizer.from_pretrained(tokenizer_dir,do_lower_case = True)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels = 2)\n",
    "            if not from_check_point:\n",
    "                self.tokenizer.add_special_tokens({'pad_token':'[PAD]'})\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "            self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "            if from_check_point:\n",
    "                config = torch.load(model_dir,map_location = {'cuda:0':\"cuda:0\"})\n",
    "                self.model.load_state_dict(config)\n",
    "\n",
    "        else:\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels = 2)\n",
    "            if from_check_point:\n",
    "                config = torch.load(model_dir)\n",
    "                self.model.load_state_dict(config)\n",
    "            #self.model = torch.load(model_dir)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name,do_lower_case = True) if from_check_point == False else AutoTokenizer.from_pretrained(tokenizer_dir,do_lower_case = True)\n",
    "        \n",
    "    def forward(self,sent,label,device):\n",
    "        token = self.tokenizer(sent, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "        output = self.model(**token,labels = label)\n",
    "\n",
    "        return output \n",
    "    \n",
    "    def save_model(self,dir):\n",
    "        self.tokenizer.save_pretrained(dir)\n",
    "        torch.save(self.model.state_dict(),dir+ f\"/dev_best_seed{self.args['seed']}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c874669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********at the epoch**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1/40:   2%|█▍                                                               | 9/425 [30:45<23:41:36, 205.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15240\\1753442247.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15240\\1753442247.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     97\u001b[0m                 tqdm(trn_loader, desc=f'epoch: {epoch + 1}/{epochs}')):  # data = (statement, trail1,trail2,label)\n\u001b[0;32m     98\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15240\\3636335552.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, sent, label, device)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max_length'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1331\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1333\u001b[1;33m         outputs = self.deberta(\n\u001b[0m\u001b[0;32m   1334\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         )\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[0;32m    540\u001b[0m                 )\n\u001b[0;32m    541\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m                 output_states = layer_module(\n\u001b[0m\u001b[0;32m    543\u001b[0m                     \u001b[0mnext_kv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m     ):\n\u001b[1;32m--> 386\u001b[1;33m         attention_output = self.attention(\n\u001b[0m\u001b[0;32m    387\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[0mrel_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m     ):\n\u001b[1;32m--> 317\u001b[1;33m         self_output = self.self(\n\u001b[0m\u001b[0;32m    318\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m         \u001b[1;31m# bsz x height x length x dimension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 760\u001b[1;33m         \u001b[0mattention_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXSoftmax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m         \u001b[0mattention_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m         context_layer = torch.bmm(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, mask, dim)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    args = {\n",
    "        'gpu':0,\n",
    "        'ptlm':'microsoft/deberta-v3-large',\n",
    "        'lmn':'deberta',\n",
    "        'data':'./training_data',\n",
    "        'epoch':40,\n",
    "        'eval_every':10,\n",
    "        'prompt':2,\n",
    "        'mode':'trn',\n",
    "        'from_check_point':False,\n",
    "        'tokenizer_dir':'./checkpoint',\n",
    "        'model_dir':'./models',\n",
    "        'seed':621\n",
    "    }\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(args['gpu'])\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "    if args['mode'] == 'trn':\n",
    "        trn_dataset = Recorddataset(args, args['data'], \"train\")\n",
    "    else:\n",
    "        trn_dataset = Recorddataset(args, args['data'], \"trn&dev\")\n",
    "    dev_dataset = Recorddataset(args, args['data'], \"dev\")\n",
    "    tst_dataset = Recorddataset(args, args['data'], \"test\")\n",
    "\n",
    "    trn_loader = DataLoader(trn_dataset, batch_size=4, shuffle=True, drop_last=False)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=1, shuffle=False, drop_last=False)\n",
    "    tst_loader = DataLoader(tst_dataset, batch_size=4, shuffle=False, drop_last=False)\n",
    "\n",
    "    seed_val = args['seed']\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    output_dir = \"./result/{}_prompt{}_mode{}_epoch{}_eval{}/\".format(args['ptlm'], args['prompt'], args['mode'], args['epoch'],\n",
    "                                                                          args['eval_every'])\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    epochs = args['epoch']\n",
    "    num_total_steps = len(trn_loader) * epochs\n",
    "    num_warmup_steps = len(trn_loader) * int(args['epoch'] / 8)\n",
    "\n",
    "    model = Model(args, args['ptlm'], args['from_check_point'], args['tokenizer_dir'], args['model_dir'])\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-6, correct_bias=True)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps,\n",
    "                                                num_training_steps=num_total_steps)\n",
    "\n",
    "    best_val, best_val_epoch = 0, 0\n",
    "    best_recall, best_precision = 0, 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for iter, (sent, label) in enumerate(\n",
    "                tqdm(trn_loader, desc=f'epoch: {epoch + 1}/{epochs}')):  # data = (statement, trail1,trail2,label)\n",
    "            label = label.to(device)\n",
    "            output = model(sent, label, device)\n",
    "            pred = torch.argmax(output[1], dim=-1)\n",
    "            total_loss += output[0].item()\n",
    "            optimizer.zero_grad()\n",
    "            output[0].backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if iter % args['eval_every'] == 0 and iter != 0:\n",
    "                with torch.no_grad():\n",
    "                    l, f, p, r = eval(model, dev_loader, device, print_on_screen=False)\n",
    "                print(\n",
    "                    f\"The Validation result at epoch {epoch + 1} iter {iter}: val_loss: {l}, val_f1: {f}, val_precision: {p}, val_recall: {r}\")\n",
    "                if f > best_val:\n",
    "                    best_val_epoch = epoch + 1\n",
    "                    best_val = f\n",
    "                    best_precision = p\n",
    "                    best_recall = r\n",
    "                    model.save_model(output_dir)\n",
    "                    Test_Results = {}\n",
    "                    for sent, uuid in tqdm(tst_loader):\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(sent, label, device)\n",
    "                            output = outputs[1]\n",
    "                            for i in range(4):\n",
    "                                if torch.argmax(output[i]) == 0:\n",
    "                                    Test_Results[str(uuid[i])] = {\"Prediction\": 'Contradiction'}\n",
    "                                else:\n",
    "                                    Test_Results[str(uuid[i])] = {\"Prediction\": \"Entailment\"}\n",
    "\n",
    "                    with open(\"{}/results.json\".format(output_dir), 'w') as jsonFile:\n",
    "                        jsonFile.write(json.dumps(Test_Results, indent=4))\n",
    "\n",
    "        print(\"total_loss_per_epoch: \", total_loss, \"best_val\", best_val, 'best_r', best_recall, 'best_p',\n",
    "              best_precision, \"best_val_epoch\", best_val_epoch)\n",
    "        if args['mode'] == 'mix':\n",
    "            Test_Results = {}\n",
    "            for (sent, uuid) in tqdm(tst_loader):\n",
    "                outputs = model(sent, label, device)\n",
    "                output = outputs[1]\n",
    "                for i in range(4):\n",
    "                    if torch.argmax(output[i]) == 0:\n",
    "                        Test_Results[str(uuid[i])] = {\"Prediction\": 'Contradiction'}\n",
    "                    else:\n",
    "                        Test_Results[str(uuid[i])] = {\"Prediction\": \"Entailment\"}\n",
    "\n",
    "            with open(\"{}/epoch{}_results.json\".format(output_dir, epoch + 1), 'w') as jsonFile:\n",
    "                jsonFile.write(json.dumps(Test_Results, indent=4))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe6a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
