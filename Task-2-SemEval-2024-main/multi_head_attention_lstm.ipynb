{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "448996fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "825f5cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Load_Preprocess():\n",
    "    def __init__(self):\n",
    "        self.Label2num = {'Entailment': 1, \"Contradiction\": 0}\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "        self.train_data = self.load_Processed_Data(self.load_Processed_cTAKES('ctakes/Train_Statements_cTAKES_Processed.json'), 'train')\n",
    "        self.dev_data = self.load_Processed_Data(self.load_Processed_cTAKES('ctakes/Dev_Statements_cTAKES_Processed.json'), 'dev')\n",
    "        self.test_data = self.load_Processed_Data(self.load_Processed_cTAKES('ctakes/Test_Statements_cTAKES_Processed.json'), 'test')\n",
    "        \n",
    "        self.statements_embeddings = 0\n",
    "        self.section_embeddings = 0\n",
    "        self.trail1_embeddings = 0\n",
    "        self.trail2_embeddings = 0\n",
    "        self.preferred_text_embeddings = 0\n",
    "\n",
    "    def load_Processed_cTAKES(self, split):\n",
    "        ctakes_tokens_path = split\n",
    "        with open(ctakes_tokens_path) as json_file:\n",
    "            ctakes_tokens = json.load(json_file)\n",
    "\n",
    "        preferred_text_dict = dict()\n",
    "        for i in range(len(ctakes_tokens)):\n",
    "            cm_dict = ctakes_tokens[i]['clinical_mention']\n",
    "            temp_list = []\n",
    "            for key in cm_dict.keys():\n",
    "                try:\n",
    "                    temp_list.append(cm_dict[key][0]['preferredText'])\n",
    "                except:\n",
    "                    continue\n",
    "            preferred_text_dict[ctakes_tokens[i]['UUID'][0]] = temp_list\n",
    "        return preferred_text_dict\n",
    "\n",
    "    def load_Processed_Data(self, ctakes, split):\n",
    "        preferred_text = []\n",
    "        statement = []\n",
    "        trail1 = []\n",
    "        trail2 = []\n",
    "        section = []\n",
    "        label = []\n",
    "        \n",
    "        with open(f\"training_data/{split}\" + \".json\") as file:\n",
    "            data = json.load(file)\n",
    "            uuid_list = list(data.keys())\n",
    "\n",
    "        for id in uuid_list:\n",
    "            statement.append(data[id]['Statement'])\n",
    "            if split != 'test':\n",
    "                label.append(self.Label2num[data[id]['Label']])\n",
    "            section.append(data[id]['Section_id'])\n",
    "        \n",
    "            with open(f\"training_data/CT json/{data[id]['Primary_id']}\" + \".json\") as file: \n",
    "                ct = json.load(file)\n",
    "                trail1.append(self.join_list(ct[data[id]['Section_id']]))\n",
    "                \n",
    "            if data[id]['Type'] == \"Comparison\":  \n",
    "                with open(f\"training_data/CT json/{data[id]['Secondary_id']}\" + \".json\") as file:\n",
    "                    ct = json.load(file)\n",
    "                    trail2.append(self.join_list(ct[data[id]['Section_id']]))\n",
    "            else:\n",
    "                trail2.append(\"_\")\n",
    "                \n",
    "            preferred_text.append(','.join(ctakes[id]))\n",
    "            \n",
    "        if split!= 'test':\n",
    "            return {'preferred_text':preferred_text, 'statement':statement, 'trail1':trail1, 'trail2':trail2, 'section':section, 'label':label}\n",
    "        else:\n",
    "            return {'preferred_text':preferred_text, 'statement':statement, 'trail1':trail1, 'trail2':trail2, 'section':section}\n",
    "    \n",
    "    def join_list(self, sentences):\n",
    "        return \", \".join([sent.strip() for sent in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "44694db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = Load_Preprocess().train_data\n",
    "test_data = Load_Preprocess().test_data\n",
    "dev_data = Load_Preprocess().dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "c1d2cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_list = train_data['label']\n",
    "dev_label_list = dev_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "04b6aceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73ced31b28f4bffbb36d36c05053855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fuzel Shaik\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Fuzel Shaik\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32c40a2786c49d6b7f2b27e30734c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff111c3b2094c19951858600153af99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c57fc52f0f454a820a5d9f43c9b333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3310f32cb21e422a991ad3dbde7e9327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import LongformerModel, AutoTokenizer\n",
    "\n",
    "# Load pre-trained model tokenizer and model\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "2318f697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preferred_text</th>\n",
       "      <th>statement</th>\n",
       "      <th>trail1</th>\n",
       "      <th>trail2</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oral cavity,Primary operation,Oral Dosage Form</td>\n",
       "      <td>All the primary trial participants do not rece...</td>\n",
       "      <td>INTERVENTION 1:, Diagnostic (FLT PET), Patient...</td>\n",
       "      <td>INTERVENTION 1:, Arm A, Patients receive oral ...</td>\n",
       "      <td>Intervention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Blood Platelets,Platelet Count measurement,Hem...</td>\n",
       "      <td>Patients with Platelet count over 100,000/mm¬¨...</td>\n",
       "      <td>DISEASE CHARACTERISTICS:, Histologically or cy...</td>\n",
       "      <td>_</td>\n",
       "      <td>Eligibility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Related personal status,Heart,Primary operation</td>\n",
       "      <td>Heart-related adverse events were recorded in ...</td>\n",
       "      <td>Adverse Events 1:, Total: 5/32 (15.63%), Febri...</td>\n",
       "      <td>Adverse Events 1:, Total: 285/752 (37.90%), An...</td>\n",
       "      <td>Adverse Events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Both breasts,Breast Carcinoma,Primary operation</td>\n",
       "      <td>Adult Patients with histologic confirmation of...</td>\n",
       "      <td>Inclusion Criteria:, Patients with histologic ...</td>\n",
       "      <td>_</td>\n",
       "      <td>Eligibility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Laser Therapy</td>\n",
       "      <td>Laser Therapy is in each cohort of the primary...</td>\n",
       "      <td>INTERVENTION 1:, Laser Therapy Alone, therapis...</td>\n",
       "      <td>INTERVENTION 1:, Part A Abemaciclib: HR+, HER2...</td>\n",
       "      <td>Intervention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>Liver function,Blood,Disease,Primary operation</td>\n",
       "      <td>Adequate blood, kidney, and hepatic function a...</td>\n",
       "      <td>Inclusion Criteria:, Postmenopausal women, Sta...</td>\n",
       "      <td>Inclusion Criteria:, Pathologically confirmed ...</td>\n",
       "      <td>Eligibility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>Primary operation,dalotuzumab</td>\n",
       "      <td>The Ridaforolimus + Dalotuzumab + Exemestane g...</td>\n",
       "      <td>Outcome Measurement:, 1. Progression-free Surv...</td>\n",
       "      <td>_</td>\n",
       "      <td>Results</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>Patient position finding,Primary operation</td>\n",
       "      <td>The only difference between the interventions ...</td>\n",
       "      <td>INTERVENTION 1:, Prone, Prone position, INTERV...</td>\n",
       "      <td>_</td>\n",
       "      <td>Intervention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>Leukocytes,White Blood Cell Count procedure</td>\n",
       "      <td>Patients must have a white blood cell count ab...</td>\n",
       "      <td>DISEASE CHARACTERISTICS:, Histologically confi...</td>\n",
       "      <td>_</td>\n",
       "      <td>Eligibility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>Neoplasms,Primary operation</td>\n",
       "      <td>the primary trial and the secondary trial both...</td>\n",
       "      <td>Outcome Measurement:, Central Nervous System (...</td>\n",
       "      <td>Outcome Measurement:, Objective Response Rate ...</td>\n",
       "      <td>Results</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1700 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         preferred_text  \\\n",
       "0        Oral cavity,Primary operation,Oral Dosage Form   \n",
       "1     Blood Platelets,Platelet Count measurement,Hem...   \n",
       "2       Related personal status,Heart,Primary operation   \n",
       "3       Both breasts,Breast Carcinoma,Primary operation   \n",
       "4                                         Laser Therapy   \n",
       "...                                                 ...   \n",
       "1695     Liver function,Blood,Disease,Primary operation   \n",
       "1696                      Primary operation,dalotuzumab   \n",
       "1697         Patient position finding,Primary operation   \n",
       "1698        Leukocytes,White Blood Cell Count procedure   \n",
       "1699                        Neoplasms,Primary operation   \n",
       "\n",
       "                                              statement  \\\n",
       "0     All the primary trial participants do not rece...   \n",
       "1     Patients with Platelet count over 100,000/mm¬¨...   \n",
       "2     Heart-related adverse events were recorded in ...   \n",
       "3     Adult Patients with histologic confirmation of...   \n",
       "4     Laser Therapy is in each cohort of the primary...   \n",
       "...                                                 ...   \n",
       "1695  Adequate blood, kidney, and hepatic function a...   \n",
       "1696  The Ridaforolimus + Dalotuzumab + Exemestane g...   \n",
       "1697  The only difference between the interventions ...   \n",
       "1698  Patients must have a white blood cell count ab...   \n",
       "1699  the primary trial and the secondary trial both...   \n",
       "\n",
       "                                                 trail1  \\\n",
       "0     INTERVENTION 1:, Diagnostic (FLT PET), Patient...   \n",
       "1     DISEASE CHARACTERISTICS:, Histologically or cy...   \n",
       "2     Adverse Events 1:, Total: 5/32 (15.63%), Febri...   \n",
       "3     Inclusion Criteria:, Patients with histologic ...   \n",
       "4     INTERVENTION 1:, Laser Therapy Alone, therapis...   \n",
       "...                                                 ...   \n",
       "1695  Inclusion Criteria:, Postmenopausal women, Sta...   \n",
       "1696  Outcome Measurement:, 1. Progression-free Surv...   \n",
       "1697  INTERVENTION 1:, Prone, Prone position, INTERV...   \n",
       "1698  DISEASE CHARACTERISTICS:, Histologically confi...   \n",
       "1699  Outcome Measurement:, Central Nervous System (...   \n",
       "\n",
       "                                                 trail2         section  \n",
       "0     INTERVENTION 1:, Arm A, Patients receive oral ...    Intervention  \n",
       "1                                                     _     Eligibility  \n",
       "2     Adverse Events 1:, Total: 285/752 (37.90%), An...  Adverse Events  \n",
       "3                                                     _     Eligibility  \n",
       "4     INTERVENTION 1:, Part A Abemaciclib: HR+, HER2...    Intervention  \n",
       "...                                                 ...             ...  \n",
       "1695  Inclusion Criteria:, Pathologically confirmed ...     Eligibility  \n",
       "1696                                                  _         Results  \n",
       "1697                                                  _    Intervention  \n",
       "1698                                                  _     Eligibility  \n",
       "1699  Outcome Measurement:, Objective Response Rate ...         Results  \n",
       "\n",
       "[1700 rows x 5 columns]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(train_data)\n",
    "df = df[['preferred_text','statement','trail1','trail2','section']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "be4b6a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(text):\n",
    "    #input_ids = tokenizer.encode(text, padding=True, truncation=True, max_length=1024, add_special_tokens=True, return_tensors='pt')\n",
    "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
    "\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
    "    global_attention_mask = torch.zeros(input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
    "    token_len = len(text.split())\n",
    "    N = int(token_len*0.1)\n",
    "    attention_list = np.linspace(0, token_len, N, endpoint=True)\n",
    "    global_attention_mask[\n",
    "        :,\n",
    "        attention_list,\n",
    "    ] = 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids,attention_mask=attention_mask, global_attention_mask=global_attention_mask)\n",
    "    return outputs.last_hidden_state[:,0,:].numpy()\n",
    "\n",
    "\n",
    "def embedder(data):\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df[['preferred_text','statement','trail1','trail2','section']]\n",
    "\n",
    "    embedding_matrix = np.zeros((df.shape[0], 5, 768))\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        preferred_text_embeddings = get_bert_embeddings(df['preferred_text'].iloc[i])\n",
    "        statement_embeddings = get_bert_embeddings(df['statement'].iloc[i])\n",
    "        trail1_embeddings = get_bert_embeddings(df['trail1'].iloc[i])\n",
    "        trail2_embeddings = get_bert_embeddings(df['trail2'].iloc[i])\n",
    "        section_embeddings = get_bert_embeddings(df['section'].iloc[i])\n",
    "        stacked_matrix = np.vstack([preferred_text_embeddings, statement_embeddings, trail1_embeddings, trail2_embeddings, section_embeddings])\n",
    "        #stacked_matrix_transpose = stacked_matrix.T\n",
    "        embedding_matrix[i, :, :] = stacked_matrix\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "bace3520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-5.22220209e-02,  9.73302871e-02, -1.63271725e-02, ...,\n",
       "         -1.09394550e-01, -2.58570928e-02, -3.53406146e-02],\n",
       "        [-4.64698449e-02,  6.45281151e-02, -4.72743511e-02, ...,\n",
       "         -9.51061398e-02, -4.10685912e-02, -5.67037761e-02],\n",
       "        [-1.99822709e-03,  5.82316816e-02, -7.34675825e-02, ...,\n",
       "          4.04712558e-03,  1.29969865e-02, -2.08499357e-02],\n",
       "        [-6.27781227e-02,  4.58259694e-02,  9.64748580e-03, ...,\n",
       "         -1.25837997e-01, -1.27292825e-02, -4.85871360e-02],\n",
       "        [-2.58054882e-02,  1.11741468e-01, -1.35565624e-02, ...,\n",
       "         -1.01113915e-01, -5.30097038e-02, -2.67949253e-02]],\n",
       "\n",
       "       [[-5.80525920e-02,  9.74157453e-02, -2.99124978e-02, ...,\n",
       "         -5.45478463e-02, -3.58684510e-02,  6.55969232e-03],\n",
       "        [-8.71125758e-02,  8.19439292e-02, -1.71879828e-02, ...,\n",
       "         -6.28861040e-03, -4.77507226e-02, -7.25038350e-02],\n",
       "        [-9.09730792e-04,  6.56911358e-02, -3.38833630e-02, ...,\n",
       "          7.46725500e-03, -3.94235253e-02, -4.31492925e-02],\n",
       "        [-3.76158208e-03,  1.18555509e-01, -3.76048870e-02, ...,\n",
       "          4.61174175e-02,  1.81028375e-03, -8.73953551e-02],\n",
       "        [-3.21501791e-02,  1.01461336e-01, -3.19597870e-03, ...,\n",
       "         -7.89234191e-02, -3.60755771e-02, -5.19929156e-02]],\n",
       "\n",
       "       [[-2.76984274e-02,  1.11614138e-01, -4.61217612e-02, ...,\n",
       "         -9.67218280e-02, -1.14953555e-02, -4.88780439e-03],\n",
       "        [-4.04682606e-02,  7.86465406e-02, -2.79774331e-02, ...,\n",
       "         -7.78573602e-02, -2.83561312e-02, -1.61650628e-02],\n",
       "        [-1.50436312e-02,  6.72923848e-02, -3.93762663e-02, ...,\n",
       "         -5.29084988e-02, -5.08801304e-02, -4.37580124e-02],\n",
       "        [-3.55906263e-02,  5.69748506e-02, -3.41470987e-02, ...,\n",
       "         -5.66225611e-02, -4.36387509e-02, -6.11982942e-02],\n",
       "        [-1.07101873e-02,  9.47592482e-02, -7.76948780e-03, ...,\n",
       "         -1.15802281e-01, -1.06994696e-02, -3.66983637e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-5.22220209e-02,  9.73302871e-02, -1.63271725e-02, ...,\n",
       "         -1.09394550e-01, -2.58570928e-02, -3.53406146e-02],\n",
       "        [-4.79842499e-02,  8.95288736e-02, -2.00743638e-02, ...,\n",
       "         -8.11699480e-02, -4.98886965e-02, -7.67400414e-02],\n",
       "        [ 2.84489244e-04,  8.23169053e-02, -4.20612767e-02, ...,\n",
       "         -2.46888362e-02, -2.35902946e-02, -2.22692639e-02],\n",
       "        [-3.54988128e-02,  6.94232136e-02, -5.74276075e-02, ...,\n",
       "         -4.80986275e-02, -1.34617323e-02, -1.90786570e-02],\n",
       "        [-2.58054882e-02,  1.11741468e-01, -1.35565624e-02, ...,\n",
       "         -1.01113915e-01, -5.30097038e-02, -2.67949253e-02]],\n",
       "\n",
       "       [[-4.89294529e-02,  1.09493144e-01, -2.42445618e-02, ...,\n",
       "         -1.19397953e-01,  6.33957621e-04, -7.55138695e-03],\n",
       "        [-3.15171778e-02,  8.40180665e-02, -5.10530919e-02, ...,\n",
       "         -1.30999565e-01, -3.96906817e-03, -1.58025920e-02],\n",
       "        [-4.28634956e-02,  9.73194614e-02, -4.96873111e-02, ...,\n",
       "          3.12333703e-02, -2.19975077e-02, -3.93369794e-02],\n",
       "        [-6.27781227e-02,  4.58259694e-02,  9.64748580e-03, ...,\n",
       "         -1.25837997e-01, -1.27292825e-02, -4.85871360e-02],\n",
       "        [-2.58054882e-02,  1.11741468e-01, -1.35565624e-02, ...,\n",
       "         -1.01113915e-01, -5.30097038e-02, -2.67949253e-02]],\n",
       "\n",
       "       [[-1.53291151e-02,  1.03013419e-01, -4.11179066e-02, ...,\n",
       "         -8.90397802e-02, -7.24047469e-03, -6.02379441e-06],\n",
       "        [-4.86117676e-02,  6.43804446e-02, -4.63753268e-02, ...,\n",
       "         -8.67237225e-02, -9.84157436e-03, -3.62922400e-02],\n",
       "        [-2.12296173e-02,  7.42838904e-02, -6.52625933e-02, ...,\n",
       "         -1.21334754e-02,  1.02603075e-03, -3.36307660e-03],\n",
       "        [-6.27781227e-02,  4.58259694e-02,  9.64748580e-03, ...,\n",
       "         -1.25837997e-01, -1.27292825e-02, -4.85871360e-02],\n",
       "        [-2.58054882e-02,  1.11741468e-01, -1.35565624e-02, ...,\n",
       "         -1.01113915e-01, -5.30097038e-02, -2.67949253e-02]]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_embedded_vector = embedder(dev_data)\n",
    "dev_embedded_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "78ee93be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1700, 5, 768)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embedded_vector = embedder(train_data)\n",
    "train_embedded_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "a34cf2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.6949, Accuracy: 0.4826, Precision: 0.4766, Recall: 0.2915, F1 Score: 0.3617\n",
      "Epoch 2/20, Train Loss: 0.6933, Accuracy: 0.4760, Precision: 0.4672, Recall: 0.2888, F1 Score: 0.3569\n",
      "Epoch 3/20, Train Loss: 0.6933, Accuracy: 0.4994, Precision: 0.4994, Recall: 1.0000, F1 Score: 0.6661\n",
      "Epoch 4/20, Train Loss: 0.6934, Accuracy: 0.4808, Precision: 0.4845, Recall: 0.6422, F1 Score: 0.5523\n",
      "Epoch 5/20, Train Loss: 0.6933, Accuracy: 0.4994, Precision: 0.4688, Recall: 0.0362, F1 Score: 0.0672\n",
      "Epoch 6/20, Train Loss: 0.6936, Accuracy: 0.4730, Precision: 0.4688, Recall: 0.3962, F1 Score: 0.4294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fuzel Shaik\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Train Loss: 0.6934, Accuracy: 0.5036, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fuzel Shaik\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Train Loss: 0.6932, Accuracy: 0.4988, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch 9/20, Train Loss: 0.6932, Accuracy: 0.4796, Precision: 0.4531, Recall: 0.2096, F1 Score: 0.2867\n",
      "Epoch 10/20, Train Loss: 0.6932, Accuracy: 0.4868, Precision: 0.4805, Recall: 0.3554, F1 Score: 0.4086\n",
      "Epoch 11/20, Train Loss: 0.6932, Accuracy: 0.4892, Precision: 0.4917, Recall: 0.5659, F1 Score: 0.5262\n",
      "Epoch 12/20, Train Loss: 0.6932, Accuracy: 0.5006, Precision: 0.5156, Recall: 0.0396, F1 Score: 0.0736\n",
      "Epoch 13/20, Train Loss: 0.6933, Accuracy: 0.4892, Precision: 0.4940, Recall: 0.7962, F1 Score: 0.6097\n",
      "Epoch 14/20, Train Loss: 0.6932, Accuracy: 0.5018, Precision: 0.5089, Recall: 0.2724, F1 Score: 0.3549\n",
      "Epoch 15/20, Train Loss: 0.6932, Accuracy: 0.4898, Precision: 0.4943, Recall: 0.8355, F1 Score: 0.6212\n",
      "Epoch 16/20, Train Loss: 0.6933, Accuracy: 0.4916, Precision: 0.4813, Recall: 0.1847, F1 Score: 0.2669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fuzel Shaik\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Train Loss: 0.6932, Accuracy: 0.5000, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch 18/20, Train Loss: 0.6932, Accuracy: 0.4868, Precision: 0.4905, Recall: 0.6791, F1 Score: 0.5696\n",
      "Epoch 19/20, Train Loss: 0.6933, Accuracy: 0.4922, Precision: 0.4781, Recall: 0.1841, F1 Score: 0.2659\n",
      "Epoch 20/20, Train Loss: 0.6932, Accuracy: 0.4784, Precision: 0.4688, Recall: 0.3245, F1 Score: 0.3835\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_lstm_layers, num_heads, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.lstm_layers = nn.ModuleList([nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True, bidirectional=True) for _ in range(num_lstm_layers)])\n",
    "\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=2*hidden_size, num_heads=num_heads)\n",
    "        \n",
    "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out = x\n",
    "        for lstm_layer in self.lstm_layers:\n",
    "            lstm_out, _ = lstm_layer(lstm_out)\n",
    "\n",
    "        lstm_out = self.relu(lstm_out) \n",
    "\n",
    "        attn_output, _ = self.self_attention(lstm_out.transpose(0, 1), lstm_out.transpose(0, 1), lstm_out.transpose(0, 1))\n",
    "        attn_output = attn_output.transpose(0, 1) \n",
    "\n",
    "        last_hidden_state = attn_output.mean(dim=1)  \n",
    "        output = self.fc(last_hidden_state)\n",
    "        return output\n",
    "\n",
    "batch_size = 64\n",
    "seq_len = 5\n",
    "input_size = 768\n",
    "output_size = 1\n",
    "\n",
    "train_tensor_data = TensorDataset(torch.from_numpy(train_embedded_vector).float(), torch.tensor(np.array(train_label_list)).float())\n",
    "train_loader = DataLoader(train_tensor_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "hidden_size = 128\n",
    "num_lstm_layers = 2\n",
    "num_heads = 4\n",
    "\n",
    "lstm_model = LSTMModel(input_size, hidden_size, num_lstm_layers, num_heads, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lstm_model.train()\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        predictions.extend(torch.sigmoid(outputs).cpu().detach().numpy() )\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    mean_loss = total_loss / len(train_loader)\n",
    "    predictions = [1 if p > 0.5 else 0 for p in predictions]\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {mean_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e198aba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6931, Accuracy: 0.5000, Precision: 0.5000, Recall: 1.0000, F1 Score: 0.6667\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            labels = labels.squeeze()\n",
    "            \n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "            true_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    mean_loss = total_loss / len(dataloader)\n",
    "    predictions = [1 if p > 0.5 else 0 for p in predictions]\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "\n",
    "    return mean_loss, accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "dev_tensor_data = TensorDataset(torch.from_numpy(dev_embedded_vector).float(), torch.tensor(np.array(dev_label_list)).float())\n",
    "dev_loader = DataLoader(dev_tensor_data, shuffle=True, drop_last=True)\n",
    "\n",
    "val_loss, val_accuracy, val_precision, val_recall, val_f1 = evaluate(lstm_model, dev_loader, criterion)\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1 Score: {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0521878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedded_vector = embedder(test_data)\n",
    "test_embedded_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f6045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a93af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad2da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc13667d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941962a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2a943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d787e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a246be76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5b176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b11d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91a57b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a852c0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "f3d3168d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6933\n",
      "Epoch 2/10, Train Loss: 0.6933\n",
      "Epoch 3/10, Train Loss: 0.6930\n",
      "Epoch 4/10, Train Loss: 0.6933\n",
      "Epoch 5/10, Train Loss: 0.6932\n",
      "Epoch 6/10, Train Loss: 0.6931\n",
      "Epoch 7/10, Train Loss: 0.6928\n",
      "Epoch 8/10, Train Loss: 0.6931\n",
      "Epoch 9/10, Train Loss: 0.6929\n",
      "Epoch 10/10, Train Loss: 0.6927\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "class BERTLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(BERTLSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_hidden_state = lstm_out[:, -1, :]\n",
    "        output = self.fc(last_hidden_state)\n",
    "        return output\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "train_tensor_data = TensorDataset(torch.from_numpy(train_embedded_vector).float(), torch.tensor(train_label_list).float())\n",
    "train_loader = DataLoader(train_tensor_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "\n",
    "input_size = 768  \n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "\n",
    "\n",
    "lstm_model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lstm_model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    mean_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {mean_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "614d630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    mean_loss = total_loss / len(dataloader)\n",
    "    predictions = [1 if p > 0.5 else 0 for p in predictions]\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    return mean_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b9ab2208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6931, Validation Accuracy: 0.5052\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "dev_tensor_data = TensorDataset(torch.from_numpy(dev_embedded_vector).float(), torch.tensor(dev_label_list).float())\n",
    "dev_loader = DataLoader(dev_tensor_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "val_loss, val_accuracy = evaluate(lstm_model, dev_loader, criterion)\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4574531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "fcbeb97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = torch.randint(0, 2, (300,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "d1bad7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "18c3bee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1700, 3840)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = train_embedded_vector.reshape((1700, -1))\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1fbd3332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10078100115060806"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1a33720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a158667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f2659e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.lstm(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "91ae6e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, learn_rate, hidden_dim=256, EPOCHS=5, model_type=\"LSTM\"):\n",
    "    \n",
    "    # Setting common hyperparameters\n",
    "    input_dim = next(iter(train_loader))[0].shape[2]\n",
    "    output_dim = 1\n",
    "    n_layers = 1\n",
    "    # Instantiating the models\n",
    "    if model_type == \"GRU\":\n",
    "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    else:\n",
    "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"Starting Training of {} model\".format(model_type))\n",
    "    epoch_times = []\n",
    "    # Start training loop\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        start_time = time.time()\n",
    "        h = model.init_hidden(batch_size)\n",
    "        avg_loss = 0.\n",
    "        counter = 0\n",
    "        for x, label in train_loader:\n",
    "            counter += 1\n",
    "            if model_type == \"GRU\":\n",
    "                h = h.data\n",
    "            else:\n",
    "                h = tuple([e.data for e in h])\n",
    "            model.zero_grad()\n",
    "            \n",
    "            out, h = model(x.to(device).float(), h)\n",
    "            loss = criterion(out, label.to(device).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            if counter%200 == 0:\n",
    "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
    "        current_time = time.time()\n",
    "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
    "        print(\"Total Time Elapsed: {} seconds\".format(str(current_time-start_time)))\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
    "    return model\n",
    "\n",
    "def evaluate(model, test_x, test_y, label_scalers):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    start_time = time.time()\n",
    "    for i in test_x.shape[0]:\n",
    "        inp = torch.from_numpy(np.array(test_x[i]))\n",
    "        labs = torch.from_numpy(np.array(test_y[i]))\n",
    "        h = model.init_hidden(inp.shape[0])\n",
    "        out, h = model(inp.to(device).float(), h)\n",
    "        outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
    "        targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
    "    print(\"Evaluation Time: {}\".format(str(time.clock()-start_time)))\n",
    "    sMAPE = 0\n",
    "    for i in range(len(outputs)):\n",
    "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
    "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
    "    return outputs, targets, sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bd6f5e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_data = TensorDataset(torch.from_numpy(train_embedded_vector), torch.from_numpy(np.array(train_label_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "88ef91de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "606450a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.3775, -0.4976, -0.9053, -0.8667, -0.4670],\n",
       "          [ 0.2264, -0.2347, -0.3397, -0.4702,  0.0931],\n",
       "          [-0.1990,  0.0105,  0.3920,  0.0222, -0.1040],\n",
       "          ...,\n",
       "          [-0.2423, -0.4277, -0.1000, -0.1339,  0.0098],\n",
       "          [ 0.0702,  0.3440,  0.0351,  0.1352,  0.0057],\n",
       "          [ 0.4793,  0.3744,  0.5974,  0.5052,  0.2998]],\n",
       " \n",
       "         [[-0.1008, -0.0921, -0.5842, -0.2127, -0.2874],\n",
       "          [-0.1103, -0.4184,  0.1822,  0.2817,  0.1403],\n",
       "          [-0.3339, -0.3547,  0.2090,  0.0348, -0.0888],\n",
       "          ...,\n",
       "          [-0.6115, -0.1707, -0.2617, -0.3424, -0.1138],\n",
       "          [-0.3047,  0.4071, -0.0997,  0.1727, -0.0028],\n",
       "          [ 0.1047,  0.5937,  0.2047,  0.2925,  0.3569]],\n",
       " \n",
       "         [[-0.1764, -0.3942, -0.1573, -0.3861, -0.5852],\n",
       "          [-0.0287, -0.5400, -0.0861,  0.0642,  0.2510],\n",
       "          [-0.2478, -0.8151, -0.3830, -0.3143, -0.4083],\n",
       "          ...,\n",
       "          [-0.1417, -0.1896, -0.1548, -0.1320, -0.3164],\n",
       "          [ 0.0489,  0.6504, -0.1846, -0.0039, -0.3954],\n",
       "          [ 0.6196,  0.3755,  0.3575,  0.0244,  0.4960]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.3428, -0.3433, -0.9239, -0.2127, -0.4670],\n",
       "          [-0.1036, -0.2304, -0.4938,  0.2817,  0.0931],\n",
       "          [-0.5165, -0.2133,  0.0946,  0.0348, -0.1040],\n",
       "          ...,\n",
       "          [-0.1176, -0.3074, -0.1946, -0.3424,  0.0098],\n",
       "          [-0.0058,  0.2906, -0.1305,  0.1727,  0.0057],\n",
       "          [ 0.5584,  0.7377,  0.8403,  0.2925,  0.2998]],\n",
       " \n",
       "         [[-0.3711, -0.1556, -0.6425, -0.2127, -0.2874],\n",
       "          [-0.3402, -0.1791,  0.2064,  0.2817,  0.1403],\n",
       "          [-0.7795, -0.1941, -0.0261,  0.0348, -0.0888],\n",
       "          ...,\n",
       "          [-0.1647, -0.1699, -0.3507, -0.3424, -0.1138],\n",
       "          [ 0.0791,  0.5068, -0.0793,  0.1727, -0.0028],\n",
       "          [ 0.5349,  0.7365,  0.2320,  0.2925,  0.3569]],\n",
       " \n",
       "         [[-0.5206, -0.4472, -0.7967, -0.7344, -0.0369],\n",
       "          [-0.2817, -0.2359, -0.2117,  0.1309,  0.1108],\n",
       "          [-0.5552, -0.1877,  0.0979,  0.0878,  0.0958],\n",
       "          ...,\n",
       "          [-0.0727, -0.3273, -0.5248, -0.3912, -0.3536],\n",
       "          [-0.0917,  0.2932,  0.2379, -0.0751, -0.0752],\n",
       "          [ 0.6324,  0.4657, -0.0307,  0.6740,  0.2748]]], dtype=torch.float64),\n",
       " tensor([0, 0, 1,  ..., 1, 1, 0], dtype=torch.int32))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2fb92981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0].shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d578c676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training of LSTM model\n",
      "Epoch 1/5 Done, Total Loss: 0.4361451268196106\n",
      "Total Time Elapsed: 29.382312059402466 seconds\n",
      "Epoch 2/5 Done, Total Loss: 0.36259178320566815\n",
      "Total Time Elapsed: 29.700926303863525 seconds\n",
      "Epoch 3/5 Done, Total Loss: 0.2799377739429474\n",
      "Total Time Elapsed: 30.123373985290527 seconds\n",
      "Epoch 4/5 Done, Total Loss: 0.27400384346644086\n",
      "Total Time Elapsed: 30.800617933273315 seconds\n",
      "Epoch 5/5 Done, Total Loss: 0.2705318530400594\n",
      "Total Time Elapsed: 29.678523302078247 seconds\n",
      "Total Training Time: 149.68575358390808 seconds\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "#gru_model = train(train_loader, lr, model_type=\"GRU\")\n",
    "Lstm_model = train(train_loader, lr, model_type=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9cbba997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lstm_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        predictions = []\n",
    "\n",
    "        for inputs, labels in zip(X_test, y_test):\n",
    "            inputs, labels = torch.tensor(inputs, dtype=torch.float32), torch.tensor(np.array(labels), dtype=torch.float32)\n",
    "            \n",
    "            h = model.init_hidden(64)\n",
    "            \n",
    "            h = tuple([e.data for e in h])\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs,h)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions.append(outputs.numpy())\n",
    "\n",
    "        mean_loss = total_loss / len(X_test)\n",
    "        print(f\"Mean Loss on Test Data: {mean_loss:.4f}\")\n",
    "\n",
    "        return np.concatenate(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b33ebcf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19716\\222960987.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_lstm_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLstm_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_embedded_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_label_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19716\\2651850400.py\u001b[0m in \u001b[0;36mevaluate_lstm_model\u001b[1;34m(model, X_test, y_test)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# Compute the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19716\\643566367.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, h)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    869\u001b[0m                         msg = (\"For unbatched 2-D input, hx and cx should \"\n\u001b[0;32m    870\u001b[0m                                f\"also be 2-D but got ({hx[0].dim()}-D, {hx[1].dim()}-D) tensors\")\n\u001b[1;32m--> 871\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m                     \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;31m# Each batch of the hidden state should match the input sequence that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors"
     ]
    }
   ],
   "source": [
    "predictions = evaluate_lstm_model(Lstm_model, dev_embedded_vector, dev_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3d3a5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, seq_len, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Extract the last time step's output\n",
    "        last_hidden_state = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layer\n",
    "        output = self.fc(last_hidden_state)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "77a64a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming you have a training dataset X_train of shape (num_samples, seq_len, input_size)\n",
    "# and corresponding labels y_train of shape (num_samples, output_size)\n",
    "\n",
    "def train_lstm_model(model, X_train, y_train, num_epochs=10, learning_rate=0.001):\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error loss for regression task\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in zip(X_train, y_train):\n",
    "            inputs, labels = torch.tensor(inputs, dtype=torch.float32), torch.tensor(np.array(labels), dtype=torch.float32)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(X_train)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "15ca08eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19716\\1983598709.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlstm_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTMModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrain_lstm_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embedded_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19716\\2081001375.py\u001b[0m in \u001b[0;36mtrain_lstm_model\u001b[1;34m(model, X_train, y_train, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;31m# Compute the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19716\\836323294.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Extract the last time step's output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mlast_hidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# Fully connected layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "input_size = 5 # Size of each input feature\n",
    "hidden_size = 128  # Number of hidden units in the LSTM\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "output_size = 1  # Size of the output\n",
    "\n",
    "lstm_model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "train_lstm_model(lstm_model, train_embedded_vector, train_label_list, num_epochs=10, learning_rate=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
